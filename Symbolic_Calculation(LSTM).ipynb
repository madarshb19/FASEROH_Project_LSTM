{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0mEJc7_CVKE"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-y19M3Rd7mp",
    "outputId": "0080c9b0-efb5-4f0f-a21f-92d330e8674a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sin(x), cos(x), exp(x), log(x + 1), atan(x), cot(x), sec(x), csc(x), asin(x), acot(x), sinh(x), cosh(x), tanh(x)]\n",
      "Function: sin(x)\n",
      "Taylor Series: -x**3/6 + x\n",
      "\n",
      "Function: cos(x)\n",
      "Taylor Series: x**4/24 - x**2/2 + 1\n",
      "\n",
      "Function: exp(x)\n",
      "Taylor Series: x**4/24 + x**3/6 + x**2/2 + x + 1\n",
      "\n",
      "Function: log(x + 1)\n",
      "Taylor Series: -x**4/4 + x**3/3 - x**2/2 + x\n",
      "\n",
      "Function: atan(x)\n",
      "Taylor Series: -x**3/3 + x\n",
      "\n",
      "Function: cot(x)\n",
      "Taylor Series: -x**3/45 - x/3 + 1/x\n",
      "\n",
      "Function: sec(x)\n",
      "Taylor Series: 5*x**4/24 + x**2/2 + 1\n",
      "\n",
      "Function: csc(x)\n",
      "Taylor Series: 7*x**3/360 + x/6 + 1/x\n",
      "\n",
      "Function: asin(x)\n",
      "Taylor Series: x**3/6 + x\n",
      "\n",
      "Function: acot(x)\n",
      "Taylor Series: x**3/3 - x + pi/2\n",
      "\n",
      "Function: sinh(x)\n",
      "Taylor Series: x**3/6 + x\n",
      "\n",
      "Function: cosh(x)\n",
      "Taylor Series: x**4/24 + x**2/2 + 1\n",
      "\n",
      "Function: tanh(x)\n",
      "Taylor Series: -x**3/3 + x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sympy.printing.str import StrPrinter\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "# Define a symbol for the independent variable\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define a list of functions to generate Taylor expansions for\n",
    "functions = [sp.sin(x), sp.cos(x), sp.exp(x), sp.ln(1+x), sp.atan(x),sp.cot(x),sp.sec(x),sp.csc(x),sp.asin(x),sp.acot(x),sp.sinh(x),sp.cosh(x),sp.tanh(x)]\n",
    "functions_str = ['sin(x)','cos(x)','exp(x)','ln(1+x)','atan(x)','cot(x)','sec(x)','csc(x)','asin(x)','acos(x)','acot(x)','sinh(x)','cosh(x)','tanh(x)'] #for tokenizing as StrPrinter() does not work directly on these functions\n",
    "\n",
    "print(functions)\n",
    "\n",
    "# Given order of expansion = 4\n",
    "order = 4\n",
    "\n",
    "# Dataset of functions and their Taylor expansions\n",
    "data = []\n",
    "for f in functions:\n",
    "    taylor_series = f.series(x, 0, order+1).removeO()\n",
    "    data.append((f, taylor_series))\n",
    "\n",
    "# Print out the dataset\n",
    "for f, t in data:\n",
    "    print(f\"Function: {f}\")\n",
    "    print(f\"Taylor Series: {t}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQh0x62FESj7"
   },
   "source": [
    "We have used these functions for Taylor expansion dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvvJAHkFCgVr"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sgb_Kgtbg647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['s', 'i', 'n', '(', 'x', ')', '-', 'x', '*', '*', '3', '/', '6', '+', 'x'],\n",
       " ['c',\n",
       "  'o',\n",
       "  's',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '4',\n",
       "  '/',\n",
       "  '2',\n",
       "  '4',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '2',\n",
       "  '/',\n",
       "  '2',\n",
       "  '+',\n",
       "  '1'],\n",
       " ['e',\n",
       "  'x',\n",
       "  'p',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '4',\n",
       "  '/',\n",
       "  '2',\n",
       "  '4',\n",
       "  '+',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '6',\n",
       "  '+',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '2',\n",
       "  '/',\n",
       "  '2',\n",
       "  '+',\n",
       "  'x',\n",
       "  '+',\n",
       "  '1'],\n",
       " ['l',\n",
       "  'n',\n",
       "  '(',\n",
       "  '1',\n",
       "  '+',\n",
       "  'x',\n",
       "  ')',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '4',\n",
       "  '/',\n",
       "  '4',\n",
       "  '+',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '3',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '2',\n",
       "  '/',\n",
       "  '2',\n",
       "  '+',\n",
       "  'x'],\n",
       " ['a',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '3',\n",
       "  '+',\n",
       "  'x'],\n",
       " ['c',\n",
       "  'o',\n",
       "  't',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '4',\n",
       "  '5',\n",
       "  '-',\n",
       "  'x',\n",
       "  '/',\n",
       "  '3',\n",
       "  '+',\n",
       "  '1',\n",
       "  '/',\n",
       "  'x'],\n",
       " ['s',\n",
       "  'e',\n",
       "  'c',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  '5',\n",
       "  '*',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '4',\n",
       "  '/',\n",
       "  '2',\n",
       "  '4',\n",
       "  '+',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '2',\n",
       "  '/',\n",
       "  '2',\n",
       "  '+',\n",
       "  '1'],\n",
       " ['c',\n",
       "  's',\n",
       "  'c',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  '7',\n",
       "  '*',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '3',\n",
       "  '6',\n",
       "  '0',\n",
       "  '+',\n",
       "  'x',\n",
       "  '/',\n",
       "  '6',\n",
       "  '+',\n",
       "  '1',\n",
       "  '/',\n",
       "  'x'],\n",
       " ['a', 's', 'i', 'n', '(', 'x', ')', 'x', '*', '*', '3', '/', '6', '+', 'x'],\n",
       " ['a',\n",
       "  'c',\n",
       "  'o',\n",
       "  's',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '3',\n",
       "  '-',\n",
       "  'x',\n",
       "  '+',\n",
       "  'p',\n",
       "  'i',\n",
       "  '/',\n",
       "  '2'],\n",
       " ['a', 'c', 'o', 't', '(', 'x', ')', 'x', '*', '*', '3', '/', '6', '+', 'x'],\n",
       " ['s',\n",
       "  'i',\n",
       "  'n',\n",
       "  'h',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '4',\n",
       "  '/',\n",
       "  '2',\n",
       "  '4',\n",
       "  '+',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '2',\n",
       "  '/',\n",
       "  '2',\n",
       "  '+',\n",
       "  '1'],\n",
       " ['c',\n",
       "  'o',\n",
       "  's',\n",
       "  'h',\n",
       "  '(',\n",
       "  'x',\n",
       "  ')',\n",
       "  '-',\n",
       "  'x',\n",
       "  '*',\n",
       "  '*',\n",
       "  '3',\n",
       "  '/',\n",
       "  '3',\n",
       "  '+',\n",
       "  'x']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a dataset of functions and their Taylor expansions\n",
    "dataset = []\n",
    "for f,g in zip(functions,functions_str):\n",
    "    # Generate the Taylor series expansion\n",
    "    taylor_series = f.series(x, 0, order+1).removeO()\n",
    "\n",
    "    # Convert the expression to a string with desired format and tokenize it\n",
    "    expr_str = StrPrinter().doprint(taylor_series).replace(' ','')\n",
    "\n",
    "    expr_fn = StrPrinter().doprint(g)\n",
    "\n",
    "    tokenized_expr = [c for c in expr_str]\n",
    "    tokenized_fn = [fn for fn in expr_fn]\n",
    "\n",
    "    tokenized_final = []\n",
    "    for i in tokenized_fn:\n",
    "      tokenized_final.append(i)\n",
    "    for j in tokenized_expr:\n",
    "      tokenized_final.append(j)\n",
    "\n",
    "    dataset.append(tokenized_final)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL2GBCEhCkUj"
   },
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDc2_QjQg4L3",
    "outputId": "4b1c1701-89ea-49a8-9f2c-e8a526adcd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s' 'i' 'n' '(' 'x' ')' '-' 'x' '*' '*' '3' '/' '6' '+' 'x' '0' '0' '0'\n",
      "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['c' 'o' 's' '(' 'x' ')' 'x' '*' '*' '4' '/' '2' '4' '-' 'x' '*' '*' '2'\n",
      "  '/' '2' '+' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['e' 'x' 'p' '(' 'x' ')' 'x' '*' '*' '4' '/' '2' '4' '+' 'x' '*' '*' '3'\n",
      "  '/' '6' '+' 'x' '*' '*' '2' '/' '2' '+' 'x' '+' '1']\n",
      " ['l' 'n' '(' '1' '+' 'x' ')' '-' 'x' '*' '*' '4' '/' '4' '+' 'x' '*' '*'\n",
      "  '3' '/' '3' '-' 'x' '*' '*' '2' '/' '2' '+' 'x' '0']\n",
      " ['a' 't' 'a' 'n' '(' 'x' ')' '-' 'x' '*' '*' '3' '/' '3' '+' 'x' '0' '0'\n",
      "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['c' 'o' 't' '(' 'x' ')' '-' 'x' '*' '*' '3' '/' '4' '5' '-' 'x' '/' '3'\n",
      "  '+' '1' '/' 'x' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['s' 'e' 'c' '(' 'x' ')' '5' '*' 'x' '*' '*' '4' '/' '2' '4' '+' 'x' '*'\n",
      "  '*' '2' '/' '2' '+' '1' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['c' 's' 'c' '(' 'x' ')' '7' '*' 'x' '*' '*' '3' '/' '3' '6' '0' '+' 'x'\n",
      "  '/' '6' '+' '1' '/' 'x' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['a' 's' 'i' 'n' '(' 'x' ')' 'x' '*' '*' '3' '/' '6' '+' 'x' '0' '0' '0'\n",
      "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['a' 'c' 'o' 's' '(' 'x' ')' 'x' '*' '*' '3' '/' '3' '-' 'x' '+' 'p' 'i'\n",
      "  '/' '2' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['a' 'c' 'o' 't' '(' 'x' ')' 'x' '*' '*' '3' '/' '6' '+' 'x' '0' '0' '0'\n",
      "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['s' 'i' 'n' 'h' '(' 'x' ')' 'x' '*' '*' '4' '/' '2' '4' '+' 'x' '*' '*'\n",
      "  '2' '/' '2' '+' '1' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['c' 'o' 's' 'h' '(' 'x' ')' '-' 'x' '*' '*' '3' '/' '3' '+' 'x' '0' '0'\n",
      "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']]\n"
     ]
    }
   ],
   "source": [
    "# Pad and convert the dataset to numpy arrays\n",
    "max_len = max(len(string) for string in dataset)\n",
    "dataset = pad_sequences(dataset, maxlen=max_len,dtype = 'str', padding='post', truncating='post',value = 0)\n",
    "dataset = np.array(dataset)\n",
    "print(dataset)\n",
    "vocab_size = len(dataset[0]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaPDbyEyCn5C"
   },
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnH5yjZq58ig",
    "outputId": "189be311-02a7-441d-cfc5-546c646e645e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '9' '1' '0' '1' '1' '4' '1' '2' '2' '7' '5' '8' '3' '1' '6' '6' '6'\n",
      "  '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6']\n",
      " ['1' '1' '1' '0' '1' '1' '1' '2' '2' '9' '5' '8' '9' '4' '1' '2' '2' '8'\n",
      "  '5' '8' '3' '7' '6' '6' '6' '6' '6' '6' '6' '6' '6']\n",
      " ['1' '1' '1' '0' '1' '1' '1' '2' '2' '8' '4' '6' '8' '3' '1' '2' '2' '7'\n",
      "  '4' '9' '3' '1' '2' '2' '6' '4' '6' '3' '1' '3' '5']\n",
      " ['1' '1' '0' '7' '3' '1' '1' '4' '1' '2' '2' '1' '5' '1' '3' '1' '2' '2'\n",
      "  '9' '5' '9' '4' '1' '2' '2' '8' '5' '8' '3' '1' '6']\n",
      " ['8' '1' '8' '9' '0' '1' '1' '4' '1' '2' '2' '7' '5' '7' '3' '1' '6' '6'\n",
      "  '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6']\n",
      " ['1' '1' '1' '0' '1' '1' '4' '1' '2' '2' '8' '5' '9' '1' '4' '1' '5' '8'\n",
      "  '3' '7' '5' '1' '6' '6' '6' '6' '6' '6' '6' '6' '6']\n",
      " ['1' '1' '1' '0' '1' '1' '9' '2' '1' '2' '2' '8' '4' '7' '8' '3' '1' '2'\n",
      "  '2' '7' '4' '7' '3' '6' '5' '5' '5' '5' '5' '5' '5']\n",
      " ['1' '1' '1' '0' '1' '1' '9' '2' '1' '2' '2' '7' '4' '7' '8' '5' '3' '1'\n",
      "  '4' '8' '3' '6' '4' '1' '5' '5' '5' '5' '5' '5' '5']\n",
      " ['8' '1' '9' '1' '0' '1' '1' '1' '2' '2' '6' '4' '7' '3' '1' '5' '5' '5'\n",
      "  '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5']\n",
      " ['9' '1' '1' '1' '0' '1' '1' '1' '2' '2' '8' '5' '8' '4' '1' '3' '1' '1'\n",
      "  '5' '7' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6']\n",
      " ['8' '9' '1' '1' '0' '1' '1' '1' '2' '2' '6' '4' '7' '3' '1' '5' '5' '5'\n",
      "  '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5']\n",
      " ['1' '1' '1' '9' '0' '1' '1' '1' '2' '2' '8' '4' '7' '8' '3' '1' '2' '2'\n",
      "  '7' '4' '7' '3' '6' '5' '5' '5' '5' '5' '5' '5' '5']\n",
      " ['8' '1' '1' '9' '0' '1' '1' '4' '1' '2' '2' '7' '5' '7' '3' '1' '6' '6'\n",
      "  '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6' '6']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "  dataset[i] = np.array(dataset[i])\n",
    "  label_encoder = LabelEncoder()\n",
    "  dataset[i] = label_encoder.fit_transform(dataset[i])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNr9bETB53DL",
    "outputId": "a939f27e-ece5-403d-8d68-7a3dad25c905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "dataset = to_categorical(dataset, num_classes=vocab_size)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W28lNRgmCuPy"
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yGgOacdWJfg"
   },
   "source": [
    "Each token is represented as a one-hot encoded vector, so the output of the model is a probability distribution over the vocabulary, indicating the likelihood of each token being the next one in the sequence. Therefore, the task can be formulated as a multi-class classification problem, where the model aims to predict the class (i.e., the next token) of the input sequence and thus,categorical_cross entropy as loss fn. can be used. 2 dropout layers are considered as to prevent overfitting which is extremely likely as the dataset,i.e,number of functions are extremely limited for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbwcNIHp50q0",
    "outputId": "818f595b-cc11-4e5c-f1a1-c62c5431811b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.4823 - val_loss: 3.4603\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4627 - val_loss: 3.4416\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.4448 - val_loss: 3.4225\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.4253 - val_loss: 3.4024\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4052 - val_loss: 3.3803\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3829 - val_loss: 3.3553\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3613 - val_loss: 3.3262\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3344 - val_loss: 3.2915\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.2978 - val_loss: 3.2491\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.2560 - val_loss: 3.1964\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2067 - val_loss: 3.1304\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1409 - val_loss: 3.0486\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0616 - val_loss: 2.9513\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9775 - val_loss: 2.8450\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8747 - val_loss: 2.7424\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7804 - val_loss: 2.6550\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7268 - val_loss: 2.5853\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6669 - val_loss: 2.5275\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6251 - val_loss: 2.4746\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5814 - val_loss: 2.4219\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5799 - val_loss: 2.3689\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5201 - val_loss: 2.3171\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5232 - val_loss: 2.2701\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4518 - val_loss: 2.2291\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.4063 - val_loss: 2.1947\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3967 - val_loss: 2.1696\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3485 - val_loss: 2.1674\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3203 - val_loss: 2.1812\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2810 - val_loss: 2.1728\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2529 - val_loss: 2.1394\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2444 - val_loss: 2.1027\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1967 - val_loss: 2.0834\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.2076 - val_loss: 2.0663\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1713 - val_loss: 2.0440\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1370 - val_loss: 2.0209\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0970 - val_loss: 2.0063\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1091 - val_loss: 1.9891\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0778 - val_loss: 1.9623\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0701 - val_loss: 1.9324\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0627 - val_loss: 1.9079\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0556 - val_loss: 1.8901\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0233 - val_loss: 1.8759\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0004 - val_loss: 1.8650\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0153 - val_loss: 1.8655\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0269 - val_loss: 1.8637\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9710 - val_loss: 1.8384\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9725 - val_loss: 1.8197\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.9453 - val_loss: 1.8116\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9253 - val_loss: 1.8181\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9263 - val_loss: 1.7970\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9114 - val_loss: 1.7750\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9394 - val_loss: 1.7620\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9250 - val_loss: 1.7501\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9093 - val_loss: 1.7631\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9127 - val_loss: 1.7231\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8507 - val_loss: 1.7124\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8508 - val_loss: 1.6841\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8481 - val_loss: 1.6918\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8634 - val_loss: 1.7079\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8626 - val_loss: 1.6785\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7846 - val_loss: 1.6772\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8009 - val_loss: 1.6310\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8143 - val_loss: 1.6800\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.8468 - val_loss: 1.5853\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.7914 - val_loss: 1.6077\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7760 - val_loss: 1.5841\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7731 - val_loss: 1.5809\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7545 - val_loss: 1.6322\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7992 - val_loss: 1.5601\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7465 - val_loss: 1.5446\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7090 - val_loss: 1.5364\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7552 - val_loss: 1.5005\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7288 - val_loss: 1.5172\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7060 - val_loss: 1.5406\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7077 - val_loss: 1.5079\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6638 - val_loss: 1.4714\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6841 - val_loss: 1.4556\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6724 - val_loss: 1.4426\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6828 - val_loss: 1.4439\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6561 - val_loss: 1.5015\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6963 - val_loss: 1.4820\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.6589 - val_loss: 1.4422\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6000 - val_loss: 1.4154\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5982 - val_loss: 1.3974\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5985 - val_loss: 1.3965\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.6448 - val_loss: 1.4122\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6149 - val_loss: 1.4245\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6080 - val_loss: 1.3938\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5974 - val_loss: 1.3565\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5659 - val_loss: 1.3456\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5928 - val_loss: 1.3367\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5608 - val_loss: 1.3288\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5744 - val_loss: 1.3405\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5640 - val_loss: 1.3448\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5802 - val_loss: 1.3281\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5468 - val_loss: 1.3206\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5413 - val_loss: 1.3021\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5469 - val_loss: 1.2920\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5524 - val_loss: 1.2961\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5561 - val_loss: 1.2956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2121b144550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(max_len, vocab_size), return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, dataset, epochs=100, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPzYIQH_ThCL",
    "outputId": "d75aeb1a-2f69-47be-858a-9effa70d84ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 570ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[4.32062075e-02, 5.38484603e-02, 3.85917164e-02, ...,\n",
       "         2.93814056e-02, 2.96735000e-02, 2.66744737e-02],\n",
       "        [5.88935986e-02, 1.10211223e-01, 5.25391772e-02, ...,\n",
       "         2.50928402e-02, 2.67015994e-02, 1.99875906e-02],\n",
       "        [7.79164508e-02, 2.82721967e-01, 7.47500658e-02, ...,\n",
       "         1.48279173e-02, 1.73124485e-02, 9.62352101e-03],\n",
       "        ...,\n",
       "        [4.56533395e-04, 6.15830952e-03, 1.30564021e-03, ...,\n",
       "         5.11325052e-05, 1.22141646e-04, 1.07306427e-04],\n",
       "        [4.54129244e-04, 6.12458307e-03, 1.29796600e-03, ...,\n",
       "         5.09966630e-05, 1.21934761e-04, 1.06892010e-04],\n",
       "        [4.52423701e-04, 6.10142155e-03, 1.29245152e-03, ...,\n",
       "         5.08907797e-05, 1.21787940e-04, 1.06600004e-04]],\n",
       "\n",
       "       [[4.32062075e-02, 5.38484603e-02, 3.85917164e-02, ...,\n",
       "         2.93814056e-02, 2.96735000e-02, 2.66744737e-02],\n",
       "        [6.21036999e-02, 1.25720814e-01, 5.38586862e-02, ...,\n",
       "         2.44825184e-02, 2.62484681e-02, 1.86902545e-02],\n",
       "        [8.06065947e-02, 3.35901707e-01, 7.53675103e-02, ...,\n",
       "         1.31570222e-02, 1.56799052e-02, 7.94171449e-03],\n",
       "        ...,\n",
       "        [5.02132752e-04, 6.73994189e-03, 1.44927239e-03, ...,\n",
       "         5.47224336e-05, 1.28133455e-04, 1.16603558e-04],\n",
       "        [4.82216303e-04, 6.47231052e-03, 1.38441182e-03, ...,\n",
       "         5.32555896e-05, 1.25635852e-04, 1.12553229e-04],\n",
       "        [4.70428669e-04, 6.31775893e-03, 1.34692888e-03, ...,\n",
       "         5.23904964e-05, 1.24178303e-04, 1.10210567e-04]],\n",
       "\n",
       "       [[4.32062075e-02, 5.38484603e-02, 3.85917164e-02, ...,\n",
       "         2.93814056e-02, 2.96735000e-02, 2.66744737e-02],\n",
       "        [6.21036999e-02, 1.25720814e-01, 5.38586862e-02, ...,\n",
       "         2.44825184e-02, 2.62484681e-02, 1.86902545e-02],\n",
       "        [8.06065947e-02, 3.35901707e-01, 7.53675103e-02, ...,\n",
       "         1.31570222e-02, 1.56799052e-02, 7.94171449e-03],\n",
       "        ...,\n",
       "        [2.63674092e-03, 3.10869552e-02, 2.54904721e-02, ...,\n",
       "         2.10955957e-04, 3.35352670e-04, 6.75464747e-04],\n",
       "        [2.46636174e-03, 2.79950500e-02, 2.36295220e-02, ...,\n",
       "         1.98642272e-04, 3.25165078e-04, 6.52064395e-04],\n",
       "        [2.30188598e-03, 2.57716291e-02, 2.13754568e-02, ...,\n",
       "         1.88899212e-04, 3.08801682e-04, 6.18749706e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3.94193381e-02, 4.42363359e-02, 3.58221792e-02, ...,\n",
       "         2.97890399e-02, 3.02737691e-02, 2.85045821e-02],\n",
       "        [4.94334921e-02, 7.40075931e-02, 4.47151512e-02, ...,\n",
       "         2.73121428e-02, 2.88274437e-02, 2.40580160e-02],\n",
       "        [6.82503134e-02, 1.77290469e-01, 6.34141490e-02, ...,\n",
       "         1.99554618e-02, 2.25440487e-02, 1.48732318e-02],\n",
       "        ...,\n",
       "        [1.48127950e-03, 1.86597891e-02, 1.04263574e-02, ...,\n",
       "         1.52276596e-04, 2.49780860e-04, 4.96434339e-04],\n",
       "        [1.46908336e-03, 1.85655970e-02, 1.03228064e-02, ...,\n",
       "         1.51418702e-04, 2.50598998e-04, 4.95361688e-04],\n",
       "        [1.45749096e-03, 1.84775088e-02, 1.02317594e-02, ...,\n",
       "         1.50575288e-04, 2.51493213e-04, 4.94458538e-04]],\n",
       "\n",
       "       [[4.32062075e-02, 5.38484603e-02, 3.85917164e-02, ...,\n",
       "         2.93814056e-02, 2.96735000e-02, 2.66744737e-02],\n",
       "        [6.21036999e-02, 1.25720814e-01, 5.38586862e-02, ...,\n",
       "         2.44825184e-02, 2.62484681e-02, 1.86902545e-02],\n",
       "        [8.06065947e-02, 3.35901707e-01, 7.53675103e-02, ...,\n",
       "         1.31570222e-02, 1.56799052e-02, 7.94171449e-03],\n",
       "        ...,\n",
       "        [1.62050221e-03, 2.00754739e-02, 1.21765239e-02, ...,\n",
       "         1.58407638e-04, 2.61459616e-04, 5.15227381e-04],\n",
       "        [1.57879142e-03, 1.94347706e-02, 1.17852269e-02, ...,\n",
       "         1.55902133e-04, 2.52820639e-04, 5.10043406e-04],\n",
       "        [1.55617634e-03, 1.91005133e-02, 1.15054911e-02, ...,\n",
       "         1.55053815e-04, 2.48024007e-04, 5.07476681e-04]],\n",
       "\n",
       "       [[3.94193381e-02, 4.42363396e-02, 3.58221792e-02, ...,\n",
       "         2.97890399e-02, 3.02737691e-02, 2.85045840e-02],\n",
       "        [5.22515029e-02, 8.40429813e-02, 4.59141135e-02, ...,\n",
       "         2.69760899e-02, 2.85815168e-02, 2.28534937e-02],\n",
       "        [7.33887777e-02, 2.20256776e-01, 6.61180764e-02, ...,\n",
       "         1.82492547e-02, 2.09748540e-02, 1.26613127e-02],\n",
       "        ...,\n",
       "        [4.58007620e-04, 6.16372516e-03, 1.31080486e-03, ...,\n",
       "         5.13715713e-05, 1.22466372e-04, 1.07618100e-04],\n",
       "        [4.55370522e-04, 6.13120804e-03, 1.30229339e-03, ...,\n",
       "         5.11762264e-05, 1.22173704e-04, 1.07148764e-04],\n",
       "        [4.53470420e-04, 6.10825745e-03, 1.29609846e-03, ...,\n",
       "         5.10296304e-05, 1.21968107e-04, 1.06813204e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These values above are the corresponding normalized probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Inferences:-\n",
    "I have tried to prevent overfitting using 2 dropout layers (Overfitting may be reduced further by k fold cross val. if it still exists).\n",
    "Although we have obtained the corresponding probability distributions by training the model, an intuitive feel of how well has the model performed is hard to visualize as the results are one hot encoded which are expected as input for the LSTM model.This one hot encoding is essential due to categorical data.\n",
    "We are also taking the functions themselves as input along with their corresponding taylor expansions to map the essential relationships between the function and the series.\n",
    "I could increase the number of functions in the dataset input to make the model more efficient,although the exact number of functions to be taken was not mentioned in the problem statement."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
